{
 "cells": [
  {
   "attachments": {
    "two-layer-net.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cs231n Two-Layer Backprop Mathematics Example\n",
    "In this notebook I attempt to connect the mathematics of backprop to the variables and network from the Two-Layer Neural Network assigment supplied here: http://cs231n.stanford.edu/assignments/2017/spring1617_assignment1.zip. Each significant variable has a section explaining the math used to calculate the variable's content in the case where the network has the following starting conditions:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathit{X} &= \\begin{bmatrix}16 & -6\\end{bmatrix} &\n",
    "\\mathit{y} &= 2 \\\\\n",
    "\\mathit{W1} &= \\begin{bmatrix}0.18 & 0.04 \\\\ 0.1 & 0.22\\end{bmatrix} &\n",
    "\\mathit{b1} &= \\begin{bmatrix}0 & 0\\end{bmatrix} \\\\\n",
    "\\mathit{W2} &= \\begin{bmatrix}0.19 & -0.1 & 0.1 \\\\ -0.02 & -0.01 & 0.04\\end{bmatrix} &\n",
    "\\mathit{b2} &= \\begin{bmatrix}0 & 0 & 0\\end{bmatrix}\n",
    "\\end{align*}$$\n",
    "\n",
    "I'll also check the calculations when possible with a completed `cs231n/classifiers/neural_net.py` that has the starting conditions hardcoded. \n",
    "\n",
    "A description of the network can be found here https://cs231n.github.io/neural-networks-case-study/#net although the variable names don't exactly match the assignment (I'm pretty sure you can figure out what's what). This diagram of the network may also help with intuitive understanding (gradients are in dark red):\n",
    "![two-layer-net.png](attachment:two-layer-net.png)\n",
    "\n",
    "**Sections**\n",
    "- [Forward Pass](#fp)\n",
    "    - [h1](#h1)\n",
    "    - [scores](#scores)\n",
    "    - [loss](#loss)\n",
    "- [Backward Pass](#bp)\n",
    "    - [dscores](#dscores)\n",
    "    - [grads\\['W2'\\]](#dW2)\n",
    "    - [grads\\['b2'\\]](#db2)\n",
    "    - [dh1](#dh1)\n",
    "    - [dh1\\[h1 <= 0\\] = 0](#dh1_1)\n",
    "    - [grads\\['W1'\\]](#dW1)\n",
    "    - [grads\\['b1'\\]](#db1)\n",
    "\n",
    "Notes\n",
    "- indexing subscripts may either start at 0 or 1, I need to fix this\n",
    "- dW1 and db1 aren't finished\n",
    "- There's some steps where I think the multivariable chain rule is required but it seems obvious the partial derivatives are 0 so I skip them. At least that's what I think is going on. For example (fixed the indexing to start at 1 here):\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2_{11}}}}\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores}} \\frac{\\partial{scores}}{\\partial{\\mathit{W2_{11}}}} \\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_1}} \\frac{\\partial{scores_1}}{\\partial{\\mathit{W2_{11}}}} +\n",
    "\\frac{\\partial{loss}}{\\partial{scores_2}} \\frac{\\partial{scores_2}}{\\partial{\\mathit{W2_{11}}}} +\n",
    "\\frac{\\partial{loss}}{\\partial{scores_3}} \\frac{\\partial{scores_3}}{\\partial{\\mathit{W2_{11}}}} \\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_1}} \\frac{\\partial{scores_1}}{\\partial{\\mathit{W2_{11}}}} + 0 + 0\\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_1}} \\frac{\\partial{scores_1}}{\\partial{\\mathit{W2_{11}}}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "X:\n",
      " [[16. -6.]] \n",
      "y:\n",
      " [2]\n",
      "W1:\n",
      "[[0.18 0.04]\n",
      " [0.1  0.22]]\n",
      "b1:\n",
      "[0. 0.]\n",
      "W2:\n",
      "[[ 0.19 -0.1   0.1 ]\n",
      " [-0.02 -0.01  0.04]]\n",
      "b2:\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "# I hard coded the starting conditions in a completed neural_net.py and renamed it backprop_math_neural_net.py\n",
    "from cs231n.classifiers.backprop_math_neural_net import TwoLayerNet\n",
    "from __future__ import print_function\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "# if we were doing a random initialization for this example,\n",
    "# these would be the arguments\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "num_classes = 3\n",
    "\n",
    "# however, since the starting conditions are hard coded the arguments don't matter much\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "X = np.array([[16., -6.]])\n",
    "y = np.array([2])\n",
    "\n",
    "# these values should match the starting conditions\n",
    "print(\"X:\\n\", X, \"\\ny:\\n\", y)\n",
    "for k in net.params:\n",
    "    print(k, \":\\n\", net.params[k], sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass <a id='fp'></a>\n",
    "***\n",
    "`h1` <a id='h1'></a>\n",
    "\n",
    "$\\mathit{h1} = \n",
    "\\max(0, \\mathit{X} \\mathit{W1} + \\mathit{b1}) = \n",
    "\\max (0, \\begin{bmatrix}16 & -6\\end{bmatrix} \\begin{bmatrix}0.18 & 0.04 \\\\ 0.1 & 0.22\\end{bmatrix} + \\begin{bmatrix}0 & 0\\end{bmatrix}) = \n",
    "\\max (0, \\begin{bmatrix}2.28 & -0.68\\end{bmatrix}) = \n",
    "\\begin{bmatrix}2.28 & 0\\end{bmatrix}$\n",
    "\n",
    "***\n",
    "`scores` <a id='scores'></a>\n",
    "\n",
    "$\\mathit{scores} = \n",
    "\\mathit{h1} \\mathit{W2} + \\mathit{b2} = \n",
    "\\begin{bmatrix}2.28 & 0\\end{bmatrix} \\begin{bmatrix}0.19 & -0.1 & 0.1 \\\\ -0.02 & -0.01 & 0.04\\end{bmatrix} + \\begin{bmatrix}0. & 0. & 0.\\end{bmatrix} = \n",
    "\\begin{bmatrix}0.4332 & -0.228 & 0.228\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network scores:\n",
      "[[ 0.4332 -0.228   0.228 ]]\n",
      "\n",
      "Math scores:\n",
      "[[ 0.4332 -0.228   0.228 ]]\n",
      "\n",
      "Difference between network scores and math scores:\n",
      "5.551115123125783e-17\n"
     ]
    }
   ],
   "source": [
    "# Check that the math matches the network\n",
    "math_scores = np.asarray([[0.4332, -0.228, 0.228]])\n",
    "scores = net.loss(X)\n",
    "print('Network scores:')\n",
    "print(scores)\n",
    "print()\n",
    "\n",
    "print('Math scores:')\n",
    "print(math_scores)\n",
    "print()\n",
    "\n",
    "print('Difference between network scores and math scores:')\n",
    "print(np.sum(np.abs(scores - math_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`loss` <a id='loss'></a>\n",
    "\n",
    "We want the true probabilities for each class. Since $y = 2$, we one hot encode to get\n",
    "$trueprob = \\begin{bmatrix}0 & 0 & 1\\end{bmatrix}$. Then\n",
    "\n",
    "$$\\begin{align}\n",
    "loss &= -(trueprob_0) \\log{(\\frac{e^{scores_0}}{e^{scores_0} + e^{scores_1} + e^{scores_2}})} \\\\\n",
    "&+ -(trueprob_1)\\log{(\\frac{e^{scores_1}}{e^{scores_0} + e^{scores_1} + e^{scores_2}})} \\\\ \n",
    "&+ -(trueprob_2)\\log{(\\frac{e^{scores_2}}{e^{scores_0} + e^{scores_1} + e^{scores_2}})} \\\\\n",
    "&= -(0)\\log{(\\frac{e^{0.4332}}{e^{0.4332} + e^{-0.228} + e^{0.228}})} \\\\\n",
    "&+ -(0)\\log{(\\frac{e^{-0.228}}{e^{0.4332} + e^{-0.228} + e^{0.228}})} \\\\\n",
    "&+ -(1)\\log{(\\frac{e^{0.228}}{e^{0.4332} + e^{-0.228} + e^{0.228}})} \\\\\n",
    "&= 1.051375468504\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network loss:\n",
      "1.0513754685043635\n",
      "\n",
      "Math loss:\n",
      "1.051375468504\n",
      "\n",
      "Difference between network loss and math loss:\n",
      "3.6348701826227625e-13\n"
     ]
    }
   ],
   "source": [
    "# Check that the math matches the network\n",
    "math_loss = 1.051375468504\n",
    "loss, grad = net.loss(X, y)\n",
    "print('Network loss:')\n",
    "print(loss)\n",
    "print()\n",
    "\n",
    "print('Math loss:')\n",
    "print(math_loss)\n",
    "print()\n",
    "\n",
    "print('Difference between network loss and math loss:')\n",
    "print(np.sum(np.abs(loss - math_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass <a id='bp'></a>\n",
    "***\n",
    "`dscores` <a id='dscores'></a>\n",
    "\n",
    "To start, we want to find $\\nabla loss = \\begin{bmatrix}\\frac{\\partial{loss}}{\\partial{scores_0}} & \\frac{\\partial{loss}}{\\partial{scores_1}}  & \\frac{\\partial{loss}}{\\partial{scores_2}}\\end{bmatrix}$ where $loss$ is a function of $scores$.\n",
    "Note that $-scores_y + \\log \\sum\\limits_{j} e^{scores_j}$ is an alternate form of the loss function that is easier to differentiate with respect to $scores_j$. Remembering that in this case $y = 2$, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{loss}}{\\partial{scores_0}} \n",
    "&= \\frac{\\partial}{\\partial{scores_0}}(-scores_2 + \\log{(e^{scores_0} + e^{scores_1} + e^{scores_2})}) \\\\\n",
    "&= 0 + \\frac{e^{scores_0}}{(e^{scores_0} + e^{scores_1} + e^{scores_2})} \\\\\n",
    "&= \\text{softmax}(scores_0), \\\\\n",
    "\\frac{\\partial{loss}}{\\partial{scores_1}} \n",
    "&= \\frac{\\partial}{\\partial{scores_1}}(-scores_2 + \\log{(e^{scores_0} + e^{scores_1} + e^{scores_2})}) \\\\\n",
    "&= 0 + \\frac{e^{scores_1}}{(e^{scores_0} + e^{scores_1} + e^{scores_2})} \\\\\n",
    "&= \\text{softmax}(scores_1), \\\\\n",
    "\\frac{\\partial{loss}}{\\partial{scores_2}} \n",
    "&= \\frac{\\partial}{\\partial{scores_2}}(-scores_2 + \\log{(e^{scores_0} + e^{scores_1} + e^{scores_2})}) \\\\\n",
    "&= -1 + \\frac{e^{scores_2}}{(e^{scores_0} + e^{scores_1} + e^{scores_2})} \\\\\n",
    "&= -1 + \\text{softmax}(scores_2). \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Next, we evaluate $\\nabla loss(scores)$ at the scores from our forward pass. Recall that $\\mathit{scores} = \\begin{bmatrix}0.4332 & -0.228 & 0.228\\end{bmatrix}$, so\n",
    "\n",
    "$$\\begin{align}\n",
    "\\nabla loss(scores) &= \\begin{bmatrix}\\text{softmax}(scores_0) & \\text{softmax}(scores_1) & -1 + \\text{softmax}(scores_2)\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\\text{softmax}(0.4332) & \\text{softmax}(-0.228) & -1 + \\text{softmax}(0.228)\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}0.429052723714 & 0.221490524653 & -0.650543248367\\end{bmatrix}.\n",
    "\\end{align}$$ \n",
    "\n",
    "This is the array stored in the variable `dscores`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`grad['W2']` <a id='dW2'></a>\n",
    "\n",
    "We want to find\n",
    "$\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2}}} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2_{11}}}} &\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2_{12}}}} &\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2_{13}}}} \\\\\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2_{21}}}} &\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2_{22}}}} &\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2_{23}}}}\n",
    "\\end{bmatrix}.\n",
    "$\n",
    "Note that\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2_{11}}}}\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_0}} \\frac{\\partial{scores_0}}{\\partial{\\mathit{W2_{11}}}} \\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_0}} \\frac{\\partial{}}{\\partial{\\mathit{W2_{11}}}}(\\mathit{h1_1} \\mathit{W2_{11}} + \\mathit{h1_2} \\mathit{W2_{21}} + \\mathit{b2_1}) \\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_0}}(\\mathit{h1_1}).\n",
    "\\end{align}$$\n",
    "\n",
    "If we apply similar logic to every element of $\\frac{\\partial{loss}}{\\partial{\\mathit{W2}}}$ we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{W2}}} \n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial{loss}}{\\partial{scores_0}}(\\mathit{h1_1}) &\n",
    "\\frac{\\partial{loss}}{\\partial{scores_1}}(\\mathit{h1_1}) &\n",
    "\\frac{\\partial{loss}}{\\partial{scores_2}}(\\mathit{h1_1}) \\\\\n",
    "\\frac{\\partial{loss}}{\\partial{scores_0}}(\\mathit{h1_2}) &\n",
    "\\frac{\\partial{loss}}{\\partial{scores_1}}(\\mathit{h1_2}) &\n",
    "\\frac{\\partial{loss}}{\\partial{scores_2}}(\\mathit{h1_2})\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} \\mathit{h1_1} \\\\ \\mathit{h1_2} \\end{bmatrix} \n",
    "\\begin{bmatrix} \\frac{\\partial{loss}}{\\partial{scores_0}} & \\frac{\\partial{loss}}{\\partial{scores_1}} & \\frac{\\partial{loss}}{\\partial{scores_2}} \\end{bmatrix} \\\\\n",
    "&= \\mathit{h1}^\\mathsf{T} \\nabla loss.\n",
    "\\end{align}$$\n",
    "\n",
    "So the array stored in `grad['W2']` is\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathit{h1}^\\mathsf{T} \\nabla loss(scores)\n",
    "&= \\begin{bmatrix}2.28 \\\\ 0\\end{bmatrix} \\begin{bmatrix}0.429052723714 & 0.221490524653 & -0.650543248367\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "0.978240210068 & 0.504998396209 & -1.483238606277 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network dW2:\n",
      "[[ 0.97824021  0.5049984  -1.48323861]\n",
      " [ 0.          0.          0.        ]]\n",
      "\n",
      "Math dW2:\n",
      "[[ 0.97824021  0.5049984  -1.48323861]\n",
      " [ 0.          0.          0.        ]]\n",
      "\n",
      "Difference between network loss and math loss:\n",
      "2.4562574196806963e-12\n"
     ]
    }
   ],
   "source": [
    "# Check that the math matches the network\n",
    "math_dW2 = np.asarray([[0.978240210068 , 0.504998396209 , -1.483238606277], [0, 0, 0]])\n",
    "print('Network dW2:')\n",
    "print(grad['W2'])\n",
    "print()\n",
    "\n",
    "print('Math dW2:')\n",
    "print(math_dW2)\n",
    "print()\n",
    "\n",
    "print('Difference between network loss and math loss:')\n",
    "print(np.sum(np.abs(grad['W2'] - math_dW2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`grad['b2']` <a id='db2'></a>\n",
    "\n",
    "We want to find\n",
    "$\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{b2}}} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{b2_{0}}}} &\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{b2_{1}}}} &\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{b2_{2}}}} \n",
    "\\end{bmatrix}.\n",
    "$\n",
    "Note that\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{b2_{0}}}}\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_0}} \\frac{\\partial{scores_0}}{\\partial{\\mathit{b2_{0}}}} \\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_0}} \\frac{\\partial{}}{\\partial{\\mathit{b2_{0}}}}(\\mathit{h1_1} \\mathit{W2_{11}} + \\mathit{h1_2} \\mathit{W2_{21}} + \\mathit{b2_0}) \\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_0}}(1).\n",
    "\\end{align}$$\n",
    "\n",
    "If we apply similar logic to every element of $\\frac{\\partial{loss}}{\\partial{\\mathit{b2}}}$ we get\n",
    "$\\frac{\\partial{loss}}{\\partial{\\mathit{b2}}} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial{loss}}{\\partial{scores_0}} &\n",
    "\\frac{\\partial{loss}}{\\partial{scores_1}} &\n",
    "\\frac{\\partial{loss}}{\\partial{scores_2}}\n",
    "\\end{bmatrix}\n",
    "= \\nabla loss(scores).$\n",
    "\n",
    "So the array stored in `grad['b2']` is\n",
    "$\\nabla loss(scores) = \\begin{bmatrix}0.429052723714 & 0.221490524653 & -0.650543248367\\end{bmatrix}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network db2:\n",
      "[ 0.42905272  0.22149052 -0.65054325]\n",
      "\n",
      "Math db2:\n",
      "[ 0.42905272  0.22149052 -0.65054325]\n",
      "\n",
      "Difference between network loss and math loss:\n",
      "9.368616993299383e-13\n"
     ]
    }
   ],
   "source": [
    "math_db2 = np.asarray([0.429052723714 , 0.221490524653 , -0.650543248367])\n",
    "print('Network db2:')\n",
    "print(grad['b2'])\n",
    "print()\n",
    "\n",
    "print('Math db2:')\n",
    "print(math_db2)\n",
    "print()\n",
    "\n",
    "print('Difference between network loss and math loss:')\n",
    "print(np.sum(np.abs(grad['b2'] - math_db2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`dh1`  <a id='dh1'></a>\n",
    "\n",
    "We want to find\n",
    "$\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1}}} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1_{1}}}} &\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1_{2}}}}\n",
    "\\end{bmatrix}.\n",
    "$\n",
    "By the multivariable chain rule\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{d{loss}}{d{\\mathit{h1_1}}} \n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_0}} \\frac{d{scores_0}}{d{\\mathit{h1_1}}} +\n",
    "\\frac{\\partial{loss}}{\\partial{scores_1}} \\frac{d{scores_1}}{d{\\mathit{h1_1}}} +\n",
    "\\frac{\\partial{loss}}{\\partial{scores_2}} \\frac{d{scores_2}}{d{\\mathit{h1_1}}} \\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_0}} \\frac{d{}}{d{\\mathit{h1_1}}}(\\mathit{h1_1} \\mathit{W2_{11}} + \\mathit{h1_2} \\mathit{W2_{21}} + \\mathit{b2_0}) +\n",
    "\\frac{\\partial{loss}}{\\partial{scores_1}} \\frac{d{}}{d{\\mathit{h1}}}(\\mathit{h1_1} \\mathit{W2_{12}} + \\mathit{h1_2} \\mathit{W2_{22}} + \\mathit{b2_1}) +\n",
    "\\frac{\\partial{loss}}{\\partial{scores_2}} \\frac{d{}}{d{\\mathit{h1}}}(\\mathit{h1_1} \\mathit{W2_{13}} + \\mathit{h1_2} \\mathit{W2_{23}} + \\mathit{b2_2}) \\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{scores_0}} (\\mathit{W2}_{11}) +\n",
    "\\frac{\\partial{loss}}{\\partial{scores_1}} (\\mathit{W2}_{12}) +\n",
    "\\frac{\\partial{loss}}{\\partial{scores_2}} (\\mathit{W2}_{13}) \\\\\n",
    "&= \\begin{bmatrix} \\frac{\\partial{loss}}{\\partial{scores_0}} & \\frac{\\partial{loss}}{\\partial{scores_1}} & \\frac{\\partial{loss}}{\\partial{scores_2}} \\end{bmatrix} \n",
    "\\begin{bmatrix} \\mathit{W2}_{11} \\\\ \\mathit{W2}_{12} \\\\ \\mathit{W2}_{13} \\end{bmatrix}.\n",
    "\\end{align}$$\n",
    "\n",
    "If we apply similar logic to every element of $\\frac{\\partial{loss}}{\\partial{\\mathit{h1}}}$ we get\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1}}} \n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "\\begin{bmatrix} \\frac{\\partial{loss}}{\\partial{scores_0}} & \\frac{\\partial{loss}}{\\partial{scores_1}} & \\frac{\\partial{loss}}{\\partial{scores_2}} \\end{bmatrix} \n",
    "\\begin{bmatrix} \\mathit{W2}_{11} \\\\ \\mathit{W2}_{12} \\\\ \\mathit{W2}_{13} \\end{bmatrix} &\n",
    "\\begin{bmatrix} \\frac{\\partial{loss}}{\\partial{scores_0}} & \\frac{\\partial{loss}}{\\partial{scores_1}} & \\frac{\\partial{loss}}{\\partial{scores_2}} \\end{bmatrix} \n",
    "\\begin{bmatrix} \\mathit{W2}_{21} \\\\ \\mathit{W2}_{22} \\\\ \\mathit{W2}_{23} \\end{bmatrix}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial{loss}}{\\partial{scores_0}} & \n",
    "\\frac{\\partial{loss}}{\\partial{scores_1}} & \n",
    "\\frac{\\partial{loss}}{\\partial{scores_2}} \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "\\mathit{W2}_{11} & \\mathit{W2}_{21} \\\\\n",
    "\\mathit{W2}_{12} & \\mathit{W2}_{22} \\\\\n",
    "\\mathit{W2}_{13} & \\mathit{W2}_{23} \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\nabla loss(scores) \\mathit{W2}^\\mathsf{T}.\n",
    "\\end{align}$$\n",
    "\n",
    "So the array stored in `dh1` is \n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1}}} \n",
    "&= \\nabla loss(scores) \\mathit{W2}^\\mathsf{T} \\\\\n",
    "&= \\begin{bmatrix}0.429052723714 & 0.221490524653 & -0.650543248367\\end{bmatrix}\n",
    "\\begin{bmatrix}0.19 & -0.02 \\\\ -0.1 & -0.01 \\\\ 0.1 & 0.04\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} -0.005683359796 & -0.036817689655 \\end{bmatrix}.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`dh1[h1 <= 0] = 0` <a id='dh1_1'></a>\n",
    "\n",
    "This is the part where we backprop through the ReLU. Let $M = X\\mathit{W1} + \\mathit{b1}$, which in this case is a $1 \\times 2$ array. What we are trying to find is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{loss}}{\\partial{M}} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial{loss}}{\\partial{M_1}} &\n",
    "\\frac{\\partial{loss}}{\\partial{M_2}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Note that\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{loss}}{\\partial{M_n}} \n",
    "&= \\frac{\\partial{loss}}{\\partial{\\mathit{h1}_n}} \n",
    "\\frac{\\partial{\\mathit{h1}_n}}{\\partial{M_n}} \\\\\n",
    "&= \\frac{\\partial{loss}}{\\partial{\\mathit{h1}_n}} \n",
    "\\frac{\\partial{}}{\\partial{M_n}} (\\max(0, M_n)) \\\\\n",
    "&= \\begin{cases}\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1}_n}}(1) = \\frac{\\partial{loss}}{\\partial{\\mathit{h1}_n}} , & \\text{if } M_n > 0\\\\\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1}_n}}(0) = 0 , & \\text{if } M_n \\leq 0.\n",
    "\\end{cases}\n",
    "\\end{align}$$\n",
    "\n",
    "So \n",
    "$\n",
    "\\frac{\\partial{loss}}{\\partial{M}} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1}_1}} \n",
    "\\frac{\\partial{\\mathit{h1}_1}}{\\partial{M_1}} &\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1}_2}} \n",
    "\\frac{\\partial{\\mathit{h1}_2}}{\\partial{M_2}}\n",
    "\\end{bmatrix}\n",
    "$. From the forward pass we know that $M = \\begin{bmatrix}2.28 & -0.68\\end{bmatrix}$. Thus we change the array stored in `dh1` to be \n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{loss}}{\\partial{M}} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1}_1}} \n",
    "\\frac{\\partial{\\mathit{h1}_1}}{\\partial{M_1}} &\n",
    "\\frac{\\partial{loss}}{\\partial{\\mathit{h1}_2}} \n",
    "\\frac{\\partial{\\mathit{h1}_2}}{\\partial{M_2}}\n",
    "\\end{bmatrix}\n",
    "&= \\begin{bmatrix} -0.005683359796(1) & -0.036817689655(0) \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} -0.005683359796 & 0 \\end{bmatrix}.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`grad['W1']` <a id='dW1'></a>\n",
    "\n",
    "wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network dW1:\n",
      "[[-0.09093376  0.        ]\n",
      " [ 0.03410016  0.        ]]\n",
      "\n",
      "Math dW1:\n",
      "0\n",
      "\n",
      "Difference between network loss and math loss:\n",
      "0.1250339155165703\n"
     ]
    }
   ],
   "source": [
    "math_dW1 = 0\n",
    "print('Network dW1:')\n",
    "print(grad['W1'])\n",
    "print()\n",
    "\n",
    "print('Math dW1:')\n",
    "print(math_dW1)\n",
    "print()\n",
    "\n",
    "print('Difference between network loss and math loss:')\n",
    "print(np.sum(np.abs(grad['W1'] - math_dW1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`grad['b1']` <a id='db1'></a>\n",
    "\n",
    "wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network db1:\n",
      "[-0.00568336  0.        ]\n",
      "\n",
      "Math db1:\n",
      "0\n",
      "\n",
      "Difference between network loss and math loss:\n",
      "0.005683359796207741\n"
     ]
    }
   ],
   "source": [
    "math_db1 = 0\n",
    "print('Network db1:')\n",
    "print(grad['b1'])\n",
    "print()\n",
    "\n",
    "print('Math db1:')\n",
    "print(math_db1)\n",
    "print()\n",
    "\n",
    "print('Difference between network loss and math loss:')\n",
    "print(np.sum(np.abs(grad['b1'] - math_db1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check\n",
    "Just to confirm `cs231n/classifiers/backprop_math_neural_net.py` is implemented correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 3.038736e-09\n",
      "b2 max relative error: 5.774141e-12\n",
      "W1 max relative error: 5.724374e-10\n",
      "b1 max relative error: 7.733046e-10\n"
     ]
    }
   ],
   "source": [
    "from cs231n.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
